{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%save MonCoreNLP.py _ih[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "serveur dans (home simph)  ./nltk-data/stanford-corenlp-full-2016-10-31/\n",
    "\n",
    "lancement\n",
    "\n",
    "java -mx6g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9009 -timeout  15000\n",
    "\n",
    "warning :\n",
    "\n",
    "il a besoin de 6G 5g de  ram pour traiter 2 langues comme en et fr\n",
    "- mettre dans la directory les models java pour chaque langue....voir le site stanford \n",
    "coreNLP server.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "import requests, logging, sys\n",
    "import pprint, json, time\n",
    "import codecs\n",
    "\n",
    "root = logging.getLogger('Root')\n",
    "root.setLevel(logging.WARNING)\n",
    "\n",
    "lhandler = logging.StreamHandler(sys.stdout)\n",
    "formatter = logging.Formatter(\n",
    "                '%(asctime)s [%(levelname)s] : %(message)s',\n",
    "                '%Y-%m-%d %H:%M:%S')\n",
    "lhandler.setFormatter(formatter)\n",
    "root.addHandler(lhandler)\n",
    "\n",
    "class CoreNLP:\n",
    "    root.debug('Object instantiating..')\n",
    "    annotator_full_list = [\"tokenize\", \"cleanxml\", \"ssplit\", \"pos\", \n",
    "    \"lemma\", \"ner\", \"regexner\", \"truecase\", \"parse\", \"depparse\", \"dcoref\", \n",
    "    \"relation\", \"natlog\", \"quote\", \"sentiment\"]\n",
    "    language_full_list = ['en', 'fr', 'ar', 'zh', 'de', 'es']\n",
    "    url = 'http://127.0.0.1:9009'\n",
    "    out_format = 'json'\n",
    "\n",
    "    def __init__(self, url=url,\n",
    "                 language = \"en\",\n",
    "                 annotator_list=annotator_full_list,\n",
    "                isPrint = False,\n",
    "                ):\n",
    "        \n",
    "        if isPrint :\n",
    "            root.setLevel(logging.DEBUG)\n",
    "            \n",
    "        \n",
    "        assert url.upper().startswith('HTTP'), \\\n",
    "            'url string should be prefixed with http'\n",
    "        \n",
    "        if 'SENTIMENT' in map(str.upper, annotator_list):\n",
    "            root.warning('You are using \"Sentiment\" annotator which is'\\\n",
    "                'not supported by Old version of CoreNLP')\n",
    "        \n",
    "        \n",
    "        if url.endswith('/'):\n",
    "            self.url = url[:-1]\n",
    "        else:\n",
    "            self.url = url\n",
    "\n",
    "        assert isinstance(annotator_list, list), \\\n",
    "            'annotators can be passed only as a python list'\n",
    "        if len(annotator_list) == 14:\n",
    "            root.info('Using all the annotators, It might take a while')\n",
    "        \n",
    "        self.annotator_list = annotator_list\n",
    "        self.language = language\n",
    "        self.isPrint = isPrint\n",
    "        common=set(self.annotator_list).intersection(self.annotator_full_list)\n",
    "        not_suprtd_elem = set(self.annotator_list) - common\n",
    "        assertion_error = 'annotator not supported: ' + str(not_suprtd_elem)\n",
    "        assert not not_suprtd_elem, assertion_error\n",
    "        \n",
    "        common = set([self.language]).intersection(self.language_full_list)\n",
    "        not_existing_language = set ( [self.language ]) - common\n",
    "        assertion_error = 'language not supported: ' + str(not_existing_language)\n",
    "        assert not not_existing_language, assertion_error\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def server_connection(current_url, data,):\n",
    "        root.debug('server connection: ' + current_url)\n",
    "        # modif PL\n",
    "        root.debug (\"data :\"+ data )\n",
    "        #print (type(data))\n",
    "        assert isinstance(data, unicode) and data, 'Enter valid string input'\n",
    "        try:\n",
    "            data_utf = data.encode('utf-8')\n",
    "        except:\n",
    "            print (repr(data))\n",
    "            raise\n",
    "        try:\n",
    "            server_out = requests.post(current_url, \n",
    "                                        data_utf, \n",
    "                                        headers={'Connection': 'close'})\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            root.error('Connection Error, check you have server running')\n",
    "            raise Exception('Check your CoreNLP Server status \\n'\n",
    "                'if not sure, Check the pywrap doc for Server instantiation')\n",
    "        return server_out\n",
    "    \n",
    "   \n",
    "    def url_calc(self, serializer=''):\n",
    "        s_string = '/?properties={\"annotators\": \"'\n",
    "        anot_string = ','.join(self.annotator_list)\n",
    "        m_string = '\", \"outputFormat\": \"' + self.out_format\n",
    "        f_string = '\", \"serializer\": \"' + serializer + '\"}&pipelineLanguage='+self.language\n",
    "        return self.url + s_string + anot_string + m_string + f_string\n",
    "\n",
    "\n",
    "    def basic(self, data, out_format='json', serializer=''):\n",
    "        self.out_format = out_format\n",
    "        format_list = ['JSON', 'XML', 'TEXT', 'SERIALIZED']\n",
    "        assert out_format.upper() in format_list, \\\n",
    "            'output format not supported, check stanford doc'\n",
    "        \n",
    "        if out_format.upper() == 'SERIALIZED' and not serializer:\n",
    "            root.info(\n",
    "                'Default Serializer is using - ' + \n",
    "                'edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer')\n",
    "            serializer = ('edu.stanford.nlp.pipeline.'\n",
    "                'ProtobufAnnotationSerializer')\n",
    "                \n",
    "        current_url = self.url_calc(serializer)\n",
    "        \n",
    "        \n",
    "        return self.server_connection(current_url, data,)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokensregex(data, pattern='', custom_filter=''):\n",
    "        root.info('TokenRegex started')\n",
    "        return CoreNLP.regex('/tokensregex', data, pattern, custom_filter)\n",
    "\n",
    "    @staticmethod\n",
    "    def semgrex(data, pattern='', custom_filter=''):\n",
    "        root.info('SemRegex started')\n",
    "        return CoreNLP.regex('/semgrex', data, pattern, custom_filter)\n",
    "\n",
    "    @staticmethod\n",
    "    def tregex(data, pattern='', custom_filter=''):\n",
    "        root.info('Tregex started')\n",
    "        return CoreNLP.regex('/tregex', data, pattern, custom_filter)\n",
    "\n",
    "    @classmethod\n",
    "    def regex(cls, endpoint, data, pattern, custom_filter):\n",
    "        url_string = '/?pattern=' + str(pattern) +'&filter=' + custom_filter \n",
    "        current_url = cls.url + endpoint + url_string\n",
    "        root.info('Returning the data requested')\n",
    "        return cls.server_connection(current_url, data)\n",
    "\n",
    "    @staticmethod\n",
    "    def process_sentences(sentences):\n",
    "        assert isinstance(sentences, list), 'it should be a list'\n",
    "        index = 0\n",
    "        new_index = 0\n",
    "        token_dict = {\n",
    "        'index':[],\n",
    "        'truecaseText':[],\n",
    "        'ner':[],\n",
    "        'before':[],\n",
    "        'originalText':[],\n",
    "        'characterOffsetBegin':[],\n",
    "        'lemma':[],\n",
    "        'truecase':[],\n",
    "        'pos':[],\n",
    "        'characterOffsetEnd':[],\n",
    "        'speaker':[],\n",
    "        'word':[],\n",
    "        'after':[],\n",
    "        'normalizedNER':[]\n",
    "        }\n",
    "        for sentence in sentences:\n",
    "            index = new_index\n",
    "            tokens = sentence['tokens']\n",
    "            for val in tokens:\n",
    "\n",
    "                #workaround to handle length inconsistancie with normalizedNER, rethink the logic\n",
    "                if 'ner' in val.keys() and 'normalizedNER' not in val.keys():\n",
    "                    token_dict['normalizedNER'].append(0)\n",
    "                    \n",
    "                for key, val in val.items():\n",
    "                    if key == 'index':\n",
    "                        new_index = index + int(val)\n",
    "                        token_dict[key].append(str(new_index))\n",
    "                    else:\n",
    "                        try:\n",
    "                            token_dict[key].append(val)\n",
    "                        except KeyError:\n",
    "                            token_dict[key] = [val]\n",
    "                            root.info('New key added: ' + key)\n",
    "        \n",
    "                         \n",
    "        return [token_dict, sentences]\n",
    "\n",
    "\n",
    "    def arrange(self, data):\n",
    "        root.info('Executing custom function')\n",
    "        assert isinstance(data, unicode) and data, 'Enter valid string input'\n",
    "        if 'lemma' not in self.annotator_list:\n",
    "            self.annotator_list.append('lemma')\n",
    "        \n",
    "        current_url = self.url_calc()\n",
    "        r = self.server_connection(current_url, data)\n",
    "        try:\n",
    "            r = r.json()\n",
    "            rs = r['sentences']\n",
    "        except ValueError:\n",
    "            root.error('Value Error: '+r.text+', Check special chars in input')\n",
    "            rs = []\n",
    "        return self.process_sentences(rs)\n",
    "    \n",
    "listePOS = \"\"\"\n",
    "\n",
    "        CC Coordinating conjunction\n",
    "        CD Cardinal number\n",
    "        DT Determiner\n",
    "        EX Existential there\n",
    "        FW Foreign word\n",
    "        IN Preposition or subordinating conjunction\n",
    "        JJ Adjective\n",
    "        JJR Adjective, comparative\n",
    "        JJS Adjective, superlative\n",
    "        LS List item marker\n",
    "        MD Modal\n",
    "        NN Noun, singular or mass\n",
    "        NNS Noun, plural\n",
    "        NNP Proper noun, singular\n",
    "        NNPS Proper noun, plural\n",
    "        PDT Predeterminer\n",
    "        POS Possessive ending\n",
    "        PRP Personal pronoun\n",
    "        PRP$ Possessive pronoun\n",
    "        RB Adverb\n",
    "        RBR Adverb, comparative\n",
    "        RBS Adverb, superlative\n",
    "        RP Particle\n",
    "        SYM Symbol\n",
    "        TO to\n",
    "        UH Interjection\n",
    "        VB Verb, base form\n",
    "        VBD Verb, past tense\n",
    "        VBG Verb, gerund or present participle\n",
    "        VBN Verb, past participle\n",
    "        VBP Verb, non­3rd person singular present\n",
    "        VBZ Verb, 3rd person singular present\n",
    "        WDT Wh­determiner\n",
    "        WP Wh­pronoun\n",
    "        WP$ Possessive wh­pronoun\n",
    "        WRB Wh­adverb\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "full_annotator_list = [\"ner\", \"pos\", \"lemma\",  \"depparse\",  \"relation\"]\n",
    "\n",
    "language =\"fr\"\n",
    "\n",
    "cf = CoreNLP(url='http://localhost:9009',\n",
    "             language = language,\n",
    "             annotator_list=full_annotator_list,\n",
    "             isPrint = False )\n",
    "\n",
    "\"\"\"\n",
    "#Calling basic function which would return a 'requests' object\n",
    "out = cf.basic(data, out_format='json'  )\n",
    "print ('Basic')haque \n",
    "pp.pprint(out.json())\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4) \n",
    "\n",
    "full_annotator_list = [\"tokenize\", \"cleanxml\", \"ssplit\", \"pos\", \"lemma\", \"ner\", \"regexner\", \"truecase\", \"parse\",\n",
    "                       \"depparse\", \"dcoref\", \"relation\", \"natlog\", \"quote\"]\n",
    "\n",
    "full_annotator_list = [\"ner\", \"pos\", \"lemma\",  \"depparse\",  \"relation\"]\n",
    "\n",
    "language =\"fr\"\n",
    "\n",
    "cf = CoreNLP(url='http://localhost:9009',\n",
    "             language = language,\n",
    "             annotator_list=full_annotator_list,\n",
    "             isPrint = False )\n",
    "\n",
    "def get_enhancedPlusPlusDependencies (data) :\n",
    "    t0 = time.time()\n",
    "    Arrange, sentences = cf.arrange(data,)\n",
    "    #print (\"temps de calcul = %1.2f secondes\" %(time.time() - t0))\n",
    "    #print (\"\\n ###################### ==== Affichage du dictionnaire Arrange ===== ###################### \\n\")\n",
    "    #pp.pprint (Arrange)\n",
    "    ner = Arrange ['ner']\n",
    "    roles = Arrange ['pos']\n",
    "    words = Arrange ['word']\n",
    "    #print (\"ner\" + str(ner))\n",
    "    \n",
    "    #print (\"\\n ###################### ==== Affichage du dictionnaire sentences ===== ###################### \\n\")\n",
    "    #pp.pprint (sentences)\n",
    "    enhancedPlusPlusDependencies = sentences[0] ['enhancedPlusPlusDependencies']\n",
    "    #print (\"\\n ## == Affichage de la valeur associée à la clé enhancedPlusPlusDependencies dans le dictionnaire sentences == ## \\n\")\n",
    "    #pp.pprint(enhancedPlusPlusDependencies)\n",
    "    \n",
    "    return enhancedPlusPlusDependencies, ner , roles, words\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def lecture_fichier(path = \"./definition de famille (simple).txt\") :\n",
    "    data = []\n",
    "    with codecs.open(path, \"r\", \"utf-8\") as f :\n",
    "        for line in f.readlines():\n",
    "            if not line.strip().startswith(\"#\") and not \"#--\" in line.strip() and not line.strip() == \"\":\n",
    "                data.append(line.strip(\"\\r\\n\"))\n",
    "\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%save affichage.py _ih[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('aaaaaaaaaaaaaaaaa', {u'dep': u'det', u'dependent': 4, u'governorGloss': u'homme', u'governor': 5, u'dependentGloss': u'un'})\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-94bca3870d11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marbre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0marbre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreationArbreDependanceNom\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0menhancedPlusPlusDependencies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-94bca3870d11>\u001b[0m in \u001b[0;36mcreationArbreDependanceNom\u001b[0;34m(enhancedPlusPlusDependencies)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0marbre\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgovernorGloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdependentGloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0marbre\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgovernorGloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdependentGloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marbre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def creationArbreDependancePosition (enhancedPlusPlusDependencies) :\n",
    "    arbre = {}\n",
    "    for group in enhancedPlusPlusDependencies :\n",
    "        dep = group [0] # 'dep' == 0\n",
    "        governor = group [3] #'governor' == 3\n",
    "        dependent = group [1] # 'dependent' == 1\n",
    "      \n",
    "        try:\n",
    "            arbre [governor].append((dependent, dep))\n",
    "        except:\n",
    "            arbre [governor]= [(dependent, dep),]\n",
    "        continue\n",
    "    return arbre\n",
    "\n",
    "\n",
    "\n",
    "def creationArbreDependanceNom (enhancedPlusPlusDependencies) :\n",
    "    arbre = {}\n",
    "    for group in enhancedPlusPlusDependencies :\n",
    "        dep = group [0] # 'dep' == 0\n",
    "        governorGloss = group [4] # 'governorGloss' == 4\n",
    "        dependentGloss = group [2] # 'dependentGloss' == 2\n",
    "        #print(\"aaaaaaaaaaaaaaaaa\", governorGloss)\n",
    "        try:\n",
    "            arbre [governorGloss].append((dependentGloss, dep))\n",
    "        except:\n",
    "            arbre [governorGloss]= [(dependentGloss, dep),]\n",
    "        continue\n",
    "    return arbre\n",
    "\n",
    "arbre = creationArbreDependanceNom (enhancedPlusPlusDependencies)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#premierMot = \"ROOT\"\n",
    "\n",
    "\n",
    "\n",
    "def affichageArbre (mot, arbre,memoire = {}) :\n",
    "    try:\n",
    "        listeDependence = arbre[mot]\n",
    "    except :\n",
    "        return\n",
    "    \n",
    "    for dependentGloss, dep in listeDependence :\n",
    "        print (mot, \"=> \" , dependentGloss,\" pour type de dependance: \", dep)\n",
    "        if not dependentGloss in memoire :\n",
    "            affichageArbre (dependentGloss, arbre, memoire = memoire)\n",
    "            memoire [dependentGloss] = True\n",
    "        continue\n",
    "    \n",
    "    return \n",
    "\n",
    "#affichageArbre (premierMot, arbre )  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%save lecture_data.py _ih[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Une personne est un homme ou une femme.', u'Une femme est f\\xe9minine.', u'Un homme est masculin.', u'Une famille a parents et enfants.', u'Un parent est un p\\xe8re ou une m\\xe8re.', u'Un enfant est un fils ou une fille.', u'Un p\\xe8re est un homme.', u'Une m\\xe8re est une femme.', u'Une jeune-fille est f\\xe9minine.', u'Un gar\\xe7on est masculin.', u'Un fils est un gar\\xe7on ou un homme.', u'Une fille est une jeune-fille ou une femme.']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%save family.py _ih[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############Une personne est un homme ou une femme.###############\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Enter valid string input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-233ab4e8f11f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mgenerer_tous_arbres\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./definition de famille (simple).txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-233ab4e8f11f>\u001b[0m in \u001b[0;36mgenerer_tous_arbres\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mpremierMoty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"ROOT\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"###############\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdatum\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"###############\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0menhanced\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_enhancedPlusPlusDependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[1;31m#print (\"&&&&&&&&&&&&&&&&&&&&&&&&& enhenced \\n\" + str(enhanced))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"&&&&&&&&&&&&&&&&&&&&&&&&& ner \\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Raja Lahiani\\Documents\\petits textes\\petits_textes_yas_raja_6\\enhanced.pyc\u001b[0m in \u001b[0;36mget_enhancedPlusPlusDependencies\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_enhancedPlusPlusDependencies\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mArrange\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[1;31m#print (\"temps de calcul = %1.2f secondes\" %(time.time() - t0))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[1;31m#print (\"\\n ###################### ==== Affichage du dictionnaire Arrange ===== ###################### \\n\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Raja Lahiani\\Documents\\petits textes\\petits_textes_yas_raja_6\\MonCoreNLP_13.pyc\u001b[0m in \u001b[0;36marrange\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0marrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Executing custom function'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Enter valid string input'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'lemma'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotator_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotator_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lemma'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Enter valid string input"
     ]
    }
   ],
   "source": [
    "import time, codecs\n",
    "#from lecture_data import lecture_fichier\n",
    "#from enhanced import get_enhancedPlusPlusDependencies\n",
    "#from affichage import creationArbreDependanceNom, affichageArbre\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nGeneralisation intemporelle et hors espace......Concept pas objet anime ou inanimé \\n\\nun chat est un carnivore\\n\\nun carnivore est un mangeur(nc) uniquement de viande.\\n\\nun mangeur est un (objet animé) qui est (mange V)\\n\\nLa regle est de generaliser les qualités du carnivore au chat par exemple\\n\\nLe chat de Merlin mange de l\\'herbe.\\n\\nréponse : impossible un chat est un carnivore donc il ne mange pas de l\\'herbe.\\n\\nretour:\\n\\nLe chat de Merlin avale de l\\'herbe pour se purger.\\n\\nréponse OK\\n\\nmot1 => \"mot general\" dico car taille faible \\net \\ninversement le retour est un Btree car beaucoup de cas possible (exemple homme = 4 milliards d\\'individus nommés )\\n\\n\\nle graphe de calcul se fait sur le mot ensuite sur le \"mot general\" et ainsi de suite...\\n\\narret des le premier résultat ?\\n\\ntous les mots concept peuvent etre potentiellement un généralisation mais pas de boucle?????\\n\\nQuand un concept se met à avoir une consistance temps et espace alors il deviend une instance par généralisable...\\n\\nUn mot peut posseder plusieurs significations au sens concept ou instanciation...\\n\\nUn mot peut avoir plusieurs sens (grouper les sens mais comment ?)\\n\\nun mot = \"dhdhddhdh\" \\n\\nune liste des sens , chaque sens est une liste d\\'objet qui est relié aux autres par mot, sens , position\\n\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Generalisation intemporelle et hors espace......Concept pas objet anime ou inanimé \n",
    "\n",
    "un chat est un carnivore\n",
    "\n",
    "un carnivore est un mangeur(nc) uniquement de viande.\n",
    "\n",
    "un mangeur est un (objet animé) qui est (mange V)\n",
    "\n",
    "La regle est de generaliser les qualités du carnivore au chat par exemple\n",
    "\n",
    "Le chat de Merlin mange de l'herbe.\n",
    "\n",
    "réponse : impossible un chat est un carnivore donc il ne mange pas de l'herbe.\n",
    "\n",
    "retour:\n",
    "\n",
    "Le chat de Merlin avale de l'herbe pour se purger.\n",
    "\n",
    "réponse OK\n",
    "\n",
    "mot1 => \"mot general\" dico car taille faible \n",
    "et \n",
    "inversement le retour est un Btree car beaucoup de cas possible (exemple homme = 4 milliards d'individus nommés )\n",
    "\n",
    "\n",
    "le graphe de calcul se fait sur le mot ensuite sur le \"mot general\" et ainsi de suite...\n",
    "\n",
    "arret des le premier résultat ?\n",
    "\n",
    "tous les mots concept peuvent etre potentiellement un généralisation mais pas de boucle?????\n",
    "\n",
    "Quand un concept se met à avoir une consistance temps et espace alors il deviend une instance par généralisable...\n",
    "\n",
    "Un mot peut posseder plusieurs significations au sens concept ou instanciation...\n",
    "\n",
    "Un mot peut avoir plusieurs sens (grouper les sens mais comment ?)\n",
    "\n",
    "un mot = \"dhdhddhdh\" \n",
    "\n",
    "une liste des sens , chaque sens est une liste d'objet qui est relié aux autres par mot, sens , position\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nCas d\\'équivallences\\n\\n\\n#######Cas 1\\n{nom propre 1} ((etre present)|(be present)) {article indéfini + nom singulier } (de | of ) {nom propre 2}\\n\\nest équivallent à \\n\\n{nom propre 2} ((avoir present) | (have present) ) {article indefini + nom singulier} (\"désigné(e)+par \" | \"called, \" )  {nom propre 1}\\n\\nExemple :\\n\\nMerlin est un fils de Patrick.\\n\\nDonne \\n\\nPatrick a un fils,nommé Merlin. \\n\\n#######Cas 2\\n(Chaque | Every ) {nom singulier 1} ((avoir present) | (have present)) {article indefini + nom singulier 2}\\n\\net\\n\\n(All | Tous | Toutes) {article defini plural + singular noun plural } (have present)) {article indefini + nom singulier 2}\\n\\nest équivallent à\\n\\n{article indefini + nom sigulier 2 } (\"is part of every\" | \"est une partie de chaque\" {nom singulier 1}\\n\\nExemple :\\n\\nEvery car have an engine.\\nAll the cars have an engine.\\n\\nequivallent\\n\\nAn engine is a part of every car.\\n\\nou\\n\\nChaque voiture a un moteur.\\nToutes les voitures ont un moteur.\\n\\néquivallent \\n\\nUn moteur est ume partie de chaque voiture.\\n\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Cas d'équivallences\n",
    "\n",
    "\n",
    "#######Cas 1\n",
    "{nom propre 1} ((etre present)|(be present)) {article indéfini + nom singulier } (de | of ) {nom propre 2}\n",
    "\n",
    "est équivallent à \n",
    "\n",
    "{nom propre 2} ((avoir present) | (have present) ) {article indefini + nom singulier} (\"désigné(e)+par \" | \"called, \" )  {nom propre 1}\n",
    "\n",
    "Exemple :\n",
    "\n",
    "Merlin est un fils de Patrick.\n",
    "\n",
    "Donne \n",
    "\n",
    "Patrick a un fils,nommé Merlin. \n",
    "\n",
    "#######Cas 2\n",
    "(Chaque | Every ) {nom singulier 1} ((avoir present) | (have present)) {article indefini + nom singulier 2}\n",
    "\n",
    "et\n",
    "\n",
    "(All | Tous | Toutes) {article defini plural + singular noun plural } (have present)) {article indefini + nom singulier 2}\n",
    "\n",
    "est équivallent à\n",
    "\n",
    "{article indefini + nom sigulier 2 } (\"is part of every\" | \"est une partie de chaque\" {nom singulier 1}\n",
    "\n",
    "Exemple :\n",
    "\n",
    "Every car have an engine.\n",
    "All the cars have an engine.\n",
    "\n",
    "equivallent\n",
    "\n",
    "An engine is a part of every car.\n",
    "\n",
    "ou\n",
    "\n",
    "Chaque voiture a un moteur.\n",
    "Toutes les voitures ont un moteur.\n",
    "\n",
    "équivallent \n",
    "\n",
    "Un moteur est ume partie de chaque voiture.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nGroupement de la connaissance dans la mémoire \\n\\n\\non connait Cas 1\\n{nom propre 1} ((etre present)|(be present)) {article indéfini + nom singulier } (de | of ) {nom propre 2}\\n\\nest équivallent à \\n\\n{nom propre 2} ((avoir present) | (have present) ) {article indefini + nom singulier} (\"désigné(e)+par \" | \"called, \" )  {nom propre 1}\\n\\nSi \\n\\n{nom propre 2} ((avoir present) | (have present) ) {article indefini + nom singulier} (\"désigné(e)+par \" | \"called, \"\\n)  {nom propre 3}\\n+\\n{nom propre 2} ((avoir present) | (have present) ) {article indefini + nom singulier} (\"désigné(e)+par \" | \"called, \" \\n)  {nom propre 1}\\n\\nAlors \\n\\n{nom propre 2} ((avoir present) | (have present) ) {article indefini + nom singulier} (\"désigné(e)+par \" | \"called, \"\\n{ nom propre 3 } (and\" | \"et\") {nom propre 1}\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Groupement de la connaissance dans la mémoire \n",
    "\n",
    "\n",
    "on connait Cas 1\n",
    "{nom propre 1} ((etre present)|(be present)) {article indéfini + nom singulier } (de | of ) {nom propre 2}\n",
    "\n",
    "est équivallent à \n",
    "\n",
    "{nom propre 2} ((avoir present) | (have present) ) {article indefini + nom singulier} (\"désigné(e)+par \" | \"called, \" )  {nom propre 1}\n",
    "\n",
    "Si \n",
    "\n",
    "{nom propre 2} ((avoir present) | (have present) ) {article indefini + nom singulier} (\"désigné(e)+par \" | \"called, \"\n",
    ")  {nom propre 3}\n",
    "+\n",
    "{nom propre 2} ((avoir present) | (have present) ) {article indefini + nom singulier} (\"désigné(e)+par \" | \"called, \" \n",
    ")  {nom propre 1}\n",
    "\n",
    "Alors \n",
    "\n",
    "{nom propre 2} ((avoir present) | (have present) ) {article indefini + nom singulier} (\"désigné(e)+par \" | \"called, \"\n",
    "{ nom propre 3 } (and\" | \"et\") {nom propre 1}\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nRaisonnement dans le temps passé vs present (+ futur ? ) / contexte\\n\\n\\n{nom propre 1} ( (be past) | (être past)) {article defini + nom singulier} of {nom propre 2}\\n\\ndevient\\n\\n(nom propre 2) ((\"have not past)| (non plus avoir past) ) {nom singulier} (anymore | )\\n\\n\\nET\\n\\n\\n{nom propre 1} ( (be past) | (être past)) {article defini | indefini + nom singulier} of {nom propre 2}\\n\\ndevient\\n\\n(nom propre 2) ((have past)| ( avoir past) ) {indefinite article + nom singulier} (\"called ,\"| \"désigné(e)+par) \\n{nom propre 1}\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Raisonnement dans le temps passé vs present (+ futur ? ) / contexte\n",
    "\n",
    "\n",
    "{nom propre 1} ( (be past) | (être past)) {article defini + nom singulier} of {nom propre 2}\n",
    "\n",
    "devient\n",
    "\n",
    "(nom propre 2) ((\"have not past)| (non plus avoir past) ) {nom singulier} (anymore | )\n",
    "\n",
    "\n",
    "ET\n",
    "\n",
    "\n",
    "{nom propre 1} ( (be past) | (être past)) {article defini | indefini + nom singulier} of {nom propre 2}\n",
    "\n",
    "devient\n",
    "\n",
    "(nom propre 2) ((have past)| ( avoir past) ) {indefinite article + nom singulier} (\"called ,\"| \"désigné(e)+par) \n",
    "{nom propre 1}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDetection des conflits et generation de questions pour resoudre le conflit\\n\\n\\n\\n(Every | chaque ) {nom singulier 1} ((be present ) | (être present) ) {article indefini + nom singular 2 }\\n( (or) | (ou)) {article indefini + nom singular 3 }\\n\\nEn conflit avec \\n\\n{nom propre 4} ((be present ) | (être present) ) {article indefini + nom singular 2 }\\n( (and) | (et) {article indefini + nom singular 3 }\\n\\nexemple :\\n\\nChaque personne est un homme ou une femme.\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Détection des conflits et génération de questions pour résoudre le conflit\n",
    "\n",
    "\n",
    "\n",
    "(Every | chaque ) {nom singulier 1} ((be present ) | (être present) ) {article indefini + nom singular 2 }\n",
    "( (or) | (ou)) {article indefini + nom singular 3 }\n",
    "\n",
    "En conflit avec \n",
    "\n",
    "{nom propre 4} ((be present ) | (être present) ) {article indefini + nom singulier 2 }\n",
    "( (and) | (et) {article indefini + nom singulier 3 }\n",
    "\n",
    "exemple :\n",
    "\n",
    "Chaque personne est un homme ou une femme.\n",
    "\n",
    "Nadia est une femme et un homme.\n",
    "\n",
    "=> conflit si Nadia est une personne.\n",
    "\n",
    "\n",
    "Regle pour raisonner :\n",
    "\n",
    "# inference\n",
    "(Every | chaque ) {nom singulier 1} ((be present ) | (être present) ) {article indefini + nom singular 2 }\n",
    "( (or) | (ou)) {article indefini + nom singular 3 }\n",
    "\n",
    "ET\n",
    "\n",
    "{nom propre 1 } ((be present ) | (être present) ) {article indefini + nom singulier 1 }\n",
    "\n",
    "=>\n",
    "\n",
    "{nom propre 1 } ((be present ) | (être present) ) {article indefini + nom singular 2 }\n",
    "( (or) | (ou)) {article indefini + nom singular 3 }\n",
    "\n",
    "\n",
    "# Question \n",
    "\n",
    "{nom propre 1 } ((be present ) | (être present) ) {article indefini + nom singular 2 }\n",
    "( (or) | (ou)) {article indefini + nom singular 3 }\n",
    "\n",
    "donne la question\n",
    "\n",
    "((be present ) | (être present) ) {nom propre 1 } {article indefini + nom singular 2 }\n",
    "( (or) | (ou)) {article indefini + nom singular 3 } ?\n",
    "\n",
    "\n",
    "\n",
    "# ou est exclusif\n",
    "\n",
    "((be present ) | (être present) ) {nom propre 1 } {article indefini + nom singular 2 }\n",
    "( (or) | (ou)) {article indefini + nom singular 3 } ?\n",
    "\n",
    "reponse :\n",
    "\n",
    "{nom propre 1 } ((be present negation ) | (être present negation ) ) {article indefini + nom singular 2 }\n",
    "\n",
    "Conclusion :\n",
    "\n",
    "{nom propre 1 } ((be present ) | (être present) )  {article indefini + nom singular 3 }\n",
    "\n",
    "\n",
    "Ou réponse :\n",
    "\n",
    "{nom propre 1 } ((be present negation ) | (être present negation ) ) {article indefini + nom singular 3 }\n",
    "\n",
    "Conclusion :\n",
    "\n",
    "{nom propre 1 } ((be present ) | (être present) )  {article indefini + nom singular 2 }\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Archivage dans le temps des faits \n",
    "(généralisation avec une variable temps dans le contexte, par défaut le temps de la machine )\n",
    "\n",
    "{nom propre 1} ((etre present)|(be present)) {article défini + nom singulier } (de | of ) {nom propre 2}\n",
    "\n",
    "équivallent à \n",
    "\n",
    "{nom propre 2} ((avoir present) | (have present) ) {article défini + nom singulier} (\"désigné(e)+par \"\n",
    "| \"called, \" )  {nom propre 1}\n",
    "\n",
    "Donc \n",
    "\n",
    "{nom propre 1} ((etre present)|(be present)) {article indéfini + nom singulier } (de | of ) {nom propre 2}\n",
    "\n",
    "SUIVI PAR \n",
    "\n",
    "{nom propre 3} ((etre present)|(be present)) {article indéfini + nom singulier } (de | of ) {nom propre 2}\n",
    "\n",
    "CONCLUSION \n",
    "\n",
    "{nom propre 2} ((avoir past) | (have past ) )  {article défini + nom singulier}  (\"désigné(e)+par \"\n",
    "| \"called, \" )  {nom propre 1}\n",
    "\n",
    "ET \n",
    "\n",
    "{nom propre 3} ((avoir present) | (have present ) )  {article défini + nom singulier}  (\"désigné(e)+par \"\n",
    "| \"called, \" )  {nom propre 2}\n",
    "\n",
    "\n",
    "EXEMPLE\n",
    "\n",
    "Bill Clinton est le president des Etats Unis.\n",
    "\n",
    "Barack Obama est le president des Etats Unis.\n",
    "\n",
    "=> \n",
    "\n",
    "The United State avait un president nommé, Bill Clinton.\n",
    "\n",
    "The United State a  un president nommé, Barack Obama.\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
